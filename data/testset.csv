,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,How can monitoring instances of human operators overriding the GAI's decisions help evaluate issues related to content provenance and maintain information integrity?,"[""Action ID: MS-4.2-004\nSuggested Action: Monitor and document instances where human operators or other systems\noverride the GAI's decisions. Evaluate these cases to understand if the overrides\nare linked to issues related to content provenance.\nGAI Risks: Information Integrity""]","Monitoring instances of human operators overriding the GAI's decisions can help evaluate issues related to content provenance and maintain information integrity by providing insights into the reasons behind such overrides. By documenting these cases and evaluating them, organizations can identify patterns or trends that may indicate weaknesses in the GAI's decision-making process or potential gaps in content provenance. This proactive approach allows for adjustments to be made to improve the overall integrity of information and decision-making processes.",simple,"[{'source': 'data/actions.csv', 'row': 168}]",True
1,"How are legal and regulatory requirements involving AI related to data collection understood, managed, and documented?","['GOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.: MAP 2.3: Scientific integrity and TEVV considerations are identified and documented, including those related to experimental\ndesign, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct\nvalidation\nAI Actor Tasks: Governance and Oversight: AI Actor Tasks: AI Development, Domain Experts, TEVV']","Legal and regulatory requirements involving AI related to data collection are understood, managed, and documented through processes that ensure compliance with laws and regulations governing data collection practices. This includes considerations such as availability, representativeness, suitability of data, and ensuring system trustworthiness and construct validation are maintained.",simple,"[{'source': 'data/tasks.csv', 'row': 18}]",True
2,What role do domain experts and relevant AI Actors play in validating the performance of AI systems in deployment context?,"['GOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.: MEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are\ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as\nintended. Results are documented.\nAI Actor Tasks: Governance and Oversight: AI Actor Tasks: AI Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV']","Domain experts and relevant AI Actors play a crucial role in validating the performance of AI systems in deployment context by providing input to measure AI system trustworthiness. Their expertise helps ensure that the system is performing consistently as intended, with results documented for transparency and accountability.",simple,"[{'source': 'data/tasks.csv', 'row': 38}]",True
3,How should dependencies between GAI and other IT or data systems be considered when updating or defining risk tiers for GAI?,"['Action ID: GV-1.3-001\nSuggested Action: Consider the following factors when updating or defining risk tiers for GAI: Abuses\nand impacts to information integrity; Dependencies between GAI and other IT or\ndata systems; Harm to fundamental rights or public safety; Presentation of\nobscene, objectionable, offensive, discriminatory, invalid or untruthful output;\nPsychological impacts to humans (e.g., anthropomorphization, algorithmic\naversion, emotional entanglement); Possibility for malicious use; Whether the\nsystem introduces significant new security vulnerabilities; Anticipated system\nimpact on some groups compared to others; Unreliable decision making\ncapabilities, validity, adaptability, and variability of GAI system performance over\ntime.\nGAI Risks: Information Integrity; Obscene,\nDegrading, and/or Abusive\nContent; Value Chain and\nComponent Integration; Harmful\nBias and Homogenization;\nDangerous, Violent, or Hateful\nContent; CBRN Information or\nCapabilities']","Dependencies between GAI and other IT or data systems should be considered when updating or defining risk tiers for GAI to assess the potential impacts on information integrity, fundamental rights, public safety, and system security. Additionally, understanding these dependencies can help in evaluating the reliability, adaptability, and performance variability of the GAI system over time.",simple,"[{'source': 'data/actions.csv', 'row': 4}]",True
4,"What is the importance of establishing policies, procedures, and processes for oversight functions across the GAI lifecycle?","['Action ID: GV-4.1-003\nSuggested Action: Establish policies, procedures, and processes for oversight functions (e.g., senior\nleadership, legal, compliance, including internal evaluation) across the GAI\nlifecycle, from problem formulation and supply chains to system decommission.\nGAI Risks: Value Chain and Component\nIntegration']","Establishing policies, procedures, and processes for oversight functions across the GAI lifecycle is crucial for ensuring effective governance, risk management, and compliance. These measures help in managing risks associated with the value chain and component integration, from problem formulation and supply chains to system decommission. They provide a structured approach to senior leadership, legal, compliance, and internal evaluation, enhancing overall accountability and transparency.",simple,"[{'source': 'data/actions.csv', 'row': 33}]",True
5,What are the risks associated with confabulation in the context of evaluating claims of model capabilities using empirically validated methods?,['Action ID: MS-2.3-002\nSuggested Action: Evaluate claims of model capabilities using empirically validated methods.\nGAI Risks: Confabulation; Information\nSecurity'],"The risks associated with confabulation in the context of evaluating claims of model capabilities using empirically validated methods include potential misinformation or false information being generated by the model. This can lead to inaccurate assessments of the model's capabilities and performance, undermining the validity of the evaluation process.",simple,"[{'source': 'data/actions.csv', 'row': 115}]",True
6,How has the Equal Employment Opportunity Commission addressed discrimination against job applicants and employees with disabilities in relation to the use of AI and automated systems?,"['**Reporting. Entities responsible for the development or use of automated systems should provide** reporting of an appropriately designed algorithmic impact assessment,[50] with clear specification of who performs the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in response to the assessment. This algorithmic impact assessment should include at least: the results of any consultation, design stage equity assessments (potentially including qualitative analysis), accessibility designs and testing, disparity testing, document any remaining disparities, and detail any mitigation implementation and assessments. This algorithmic impact assessment should be made public whenever possible. Reporting should be provided in a clear and machine-readable manner using plain language to allow for more straightforward public accountability. **Algorithmic** **Discrimination** **Protections**\n\n###### HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\n\n Real-life examples of how these principles can become reality, through laws, policies, and practical technical and sociotechnical approaches to protecting rights, opportunities, and access.\n\n**The federal government is working to combat discrimination in mortgage lending. The Depart\xad** ment of Justice has launched a nationwide initiative to combat redlining, which includes reviewing how [lenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.[51]](https://advertising.51)\n\nThis initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial Protection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation Equity includes a commitment from the agencies that oversee mortgage lending to include a [nondiscrimination standard in the proposed rules for Automated Valuation Models.[52]](https://Models.52)\n\n**The Equal** **Employment** **Opportunity** **Commission and the Department** **of Justice** **have** **clearly** **laid out how employers’ use of AI and other automated systems can result in** **[discrimination against job applicants and employees with disabilities.[53]](https://disabilities.53)** The documents explain how employers’ use of software that relies on algorithmic decision-making may violate existing requirements under Title I of the Americans with Disabilities Act (“ADA”). This technical assistance also provides practical tips to employers on how to comply with the ADA, and to job applicants and employees who think that their rights may have been violated.']","The Equal Employment Opportunity Commission and the Department of Justice have clearly laid out how employers’ use of AI and other automated systems can result in discrimination against job applicants and employees with disabilities. The documents explain how employers’ use of software that relies on algorithmic decision-making may violate existing requirements under Title I of the Americans with Disabilities Act (“ADA”). This technical assistance also provides practical tips to employers on how to comply with the ADA, and to job applicants and employees who think that their rights may have been violated.",simple,[{}],True
7,"What risks should be considered when establishing policies for the collection, retention, and minimum quality of data?","['Action ID: MP-4.1-005\nSuggested Action: Establish policies for collection, retention, and minimum quality of data, in\nconsideration of the following risks: Disclosure of inappropriate CBRN information;\nUse of Illegal or dangerous content; Offensive cyber capabilities; Training data\nimbalances that could give rise to harmful biases; Leak of personally identifiable\ninformation, including facial likenesses of individuals.\nGAI Risks: CBRN Information or Capabilities;\nIntellectual Property; Information\nSecurity; Harmful Bias and\nHomogenization; Dangerous,\nViolent, or Hateful Content; Data\nPrivacy']","Disclosure of inappropriate CBRN information; Use of Illegal or dangerous content; Offensive cyber capabilities; Training data imbalances that could give rise to harmful biases; Leak of personally identifiable information, including facial likenesses of individuals.",simple,"[{'source': 'data/actions.csv', 'row': 84}]",True
8,What is the purpose of performing AI red-teaming in assessing resilience against various types of attacks in the context of information security and harmful content?,"['Action ID: MS-2.7-007\nSuggested Action: Perform AI red-teaming to assess resilience against: Abuse to facilitate attacks on\nother systems (e.g., malicious code generation, enhanced phishing content), GAI\nattacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts,\ndata poisoning, membership inference, model extraction, sponge examples).\nGAI Risks: Information Security; Harmful Bias\nand Homogenization; Dangerous,\nViolent, or Hateful Content', 'Action ID: MS-2.10-001\nSuggested Action: Conduct AI red-teaming to assess issues such as: Outputting of training data\nsamples, and subsequent reverse engineering, model extraction, and\nmembership inference risks; Revealing biometric, confidential, copyrighted,\nlicensed, patented, personal, proprietary, sensitive, or trade-marked information;\nTracking or revealing location information of users or members of training\ndatasets.\nGAI Risks: Human-AI Configuration;\nInformation Integrity; Intellectual\nProperty']","The purpose of performing AI red-teaming in assessing resilience against various types of attacks in the context of information security and harmful content is to identify vulnerabilities and weaknesses in AI systems that could be exploited by malicious actors. By simulating real-world attack scenarios, red-teaming helps organizations understand their security posture and improve their defenses against threats such as abuse to facilitate attacks on other systems, GAI attacks, ML attacks, and risks related to information security, harmful bias, and dangerous content.",simple,"[{'source': 'data/actions.csv', 'row': 137}, {'source': 'data/actions.csv', 'row': 146}]",True
9,What are the suggested actions for providing input on the capabilities and limitations of GAI systems related to digital content transparency?,"['Action ID: MS-3.3-004\nSuggested Action: Provide input for training materials about the capabilities and limitations of GAI\nsystems related to digital content transparency for AI Actors, other\nprofessionals, and the public about the societal impacts of AI and the role of\ndiverse and inclusive content generation.\nGAI Risks: Human-AI Configuration;\nInformation Integrity; Harmful Bias\nand Homogenization']","Provide input for training materials about the capabilities and limitations of GAI systems related to digital content transparency for AI Actors, other professionals, and the public about the societal impacts of AI and the role of diverse and inclusive content generation.",simple,"[{'source': 'data/actions.csv', 'row': 163}]",True
10,"What factors should be considered when updating risk tiers for GAI, including information integrity, IT system dependencies, fundamental rights, objectionable content, psychological impacts, malicious use, security vulnerabilities, impact on groups, and decision-making capabilities?","['Action ID: GV-1.3-001\nSuggested Action: Consider the following factors when updating or defining risk tiers for GAI: Abuses\nand impacts to information integrity; Dependencies between GAI and other IT or\ndata systems; Harm to fundamental rights or public safety; Presentation of\nobscene, objectionable, offensive, discriminatory, invalid or untruthful output;\nPsychological impacts to humans (e.g., anthropomorphization, algorithmic\naversion, emotional entanglement); Possibility for malicious use; Whether the\nsystem introduces significant new security vulnerabilities; Anticipated system\nimpact on some groups compared to others; Unreliable decision making\ncapabilities, validity, adaptability, and variability of GAI system performance over\ntime.\nGAI Risks: Information Integrity; Obscene,\nDegrading, and/or Abusive\nContent; Value Chain and\nComponent Integration; Harmful\nBias and Homogenization;\nDangerous, Violent, or Hateful\nContent; CBRN Information or\nCapabilities']","Consider the following factors when updating or defining risk tiers for GAI: Abuses and impacts to information integrity; Dependencies between GAI and other IT or data systems; Harm to fundamental rights or public safety; Presentation of obscene, objectionable, offensive, discriminatory, invalid or untruthful output; Psychological impacts to humans (e.g., anthropomorphization, algorithmic aversion, emotional entanglement); Possibility for malicious use; Whether the system introduces significant new security vulnerabilities; Anticipated system impact on some groups compared to others; Unreliable decision making capabilities, validity, adaptability, and variability of GAI system performance over time.",multi_context,"[{'source': 'data/actions.csv', 'row': 4}]",True
11,How can AI red-teamers improve GAI model testing?,"['• Expert: Performed by specialists with expertise in the domain or specific AI red-teaming context of use (e.g., medicine, biotech, cybersecurity). \n\n • Combination: In scenarios when it is difficult to identify and recruit specialists with sufficient domain and contextual expertise, AI red-teaming exercises may leverage both expert and #### general public participants. For example, expert AI red-teamers could modify or verify the prompts written by general public AI red-teamers. These approaches may also expand coverage of the AI risk attack surface. \n\n • Human / AI: Performed by GAI in combination with specialist or non-specialist human teams. GAI-led red-teaming can be more cost effective than human red-teamers alone. Human or GAI- led AI red-teaming may be better suited for eliciting different types of harms.\n\n A.1.6. Content Provenance\n\n Overview\n\n GAI technologies can be leveraged for many applications such as content generation and synthetic data. Some aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to distinguish human-generated content from AI-generated synthetic content. To help manage and mitigate these risks, digital transparency mechanisms like provenance data tracking can trace the origin and history of content. Provenance data tracking and synthetic content detection can help facilitate greater information access about both authentic and synthetic content to users, enabling better knowledge of trustworthiness in AI systems. When combined with other organizational accountability mechanisms, digital content transparency approaches can enable processes to trace negative outcomes back to their source, improve information integrity, and uphold public trust. Provenance data tracking and synthetic content detection mechanisms provide information about the origin and history of content to assist in GAI risk management efforts.\n\n Provenance metadata can include information about GAI model developers or creators of GAI content, date/time of creation, location, modifications, and sources. Metadata can be tracked for text, images, videos, audio, and underlying datasets. The implementation of provenance data tracking techniques can help assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital content. Some well-known techniques for provenance data tracking include digital watermarking, metadata recording, digital fingerprinting, and human authentication, among others.\n\n Provenance Data Tracking Approaches']","AI red-teamers can improve GAI model testing by leveraging a combination of expert and general public participants in red-teaming exercises. This approach allows for the modification or verification of prompts written by general public AI red-teamers by expert AI red-teamers, expanding the coverage of the AI risk attack surface. Additionally, GAI-led red-teaming, either in combination with specialist or non-specialist human teams, can be more cost-effective than human red-teamers alone, and may be better suited for eliciting different types of harms. By utilizing provenance data tracking techniques, AI red-teamers can assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital content, thereby improving GAI model testing.",multi_context,[{}],True
12,"Why is parental notification important in child welfare investigations, especially with automated systems?","['- A formal child welfare investigation is opened against a parent based on an algorithm and without the parent ever being notified that data was being collected and used as part of an algorithmic child maltreatment [risk assessment.[84]](https://assessment.84) The lack of notice or an explanation makes it harder for those performing child maltreatment assessments to validate the risk assessment and denies parents knowledge that could help them contest a decision. **NOTICE &** **EXPLANATION**\n\n###### WHY THIS PRINCIPLE IS IMPORTANT\n\n This section provides a brief summary of the problems which the principle seeks to address and protect against, including illustrative examples.\n\n- A predictive policing system claimed to identify individuals at greatest risk to commit or become the victim of gun violence (based on automated analysis of social ties to gang members, criminal histories, previous experi\xad ences of gun violence, and other factors) and led to individuals being placed on a watch list with no [explanation or public transparency regarding how the system came to its conclusions.[85]](https://conclusions.85) Both police and the public deserve to understand why and how such a system is making these determinations.\n\n- A system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entry errors and other system flaws. These flaws were only revealed when an explanation of the system [was demanded and produced.[86]](https://produced.86) The lack of an explanation made it harder for errors to be corrected in a timely manner. **NOTICE &** **EXPLANATION**\n\n###### WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\n\n The expectations for automated systems are meant to serve as a blueprint for the development of additional technical standards and practices that are tailored for particular sectors and contexts.\n\nAn automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and explanations as to how and why a decision was made or an action was taken by the system. These expectations are explained below.\n\n**Provide clear, timely, understandable, and accessible notice of use and explanations \xad**', 'You should know that an automated system is being used, and understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of automat\xad ed systems should provide generally accessible plain language docu\xad mentation including clear descriptions of the overall system func\xad tioning and the role automation plays, notice that such systems are in use, the individual or organization responsible for the system, and ex\xad planations of outcomes that are clear, timely, and accessible. Such notice should be kept up-to-date and people impacted by the system should be notified of significant use case or key functionality chang\xad es. You should know how and why an outcome impacting you was de\xad termined by an automated system, including when the automated system is not the sole input determining the outcome. Automated systems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context. Reporting that includes summary information about these automated systems in plain language and assessments of the clarity and quality of the notice and explanations should be made public whenever possible. \n\n###### XPLANATION **NOTICE &** **EXPLANATION**\n\n###### WHY THIS PRINCIPLE IS IMPORTANT\n\n This section provides a brief summary of the problems which the principle seeks to address and protect against, including illustrative examples.\n\nAutomated systems now determine opportunities, from employment to credit, and directly shape the American public’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives. But this expansive impact is not always visible. An applicant might not know whether a person rejected their resume or a hiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny\xad ing their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contesting decisions, people are often denied the knowledge they need to address the impact of automated systems on their lives. Notice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable\xad ness of a recommendation before enacting it.']","Parental notification is important in child welfare investigations, especially with automated systems, because it allows parents to be aware that data is being collected and used as part of an algorithmic risk assessment. This notification provides parents with the opportunity to contest a decision and validate the risk assessment process. Without proper notice, parents are denied the knowledge that could help them understand and address the implications of the investigation.",multi_context,"[{}, {}]",True
13,How to address CBRN threats in GAI system development?,"['Action ID: GV-1.3-007\nSuggested Action: Devise a plan to halt development or deployment of a GAI system that poses\nunacceptable negative risk.\nGAI Risks: CBRN Information and Capability;\nInformation Security; Information\nIntegrity', 'Action ID: GV-3.2-005\nSuggested Action: Engage in threat modeling to anticipate potential risks from GAI systems.\nGAI Risks: CBRN Information or Capabilities;\nInformation Security']",The answer to given question is not present in context,multi_context,"[{'source': 'data/actions.csv', 'row': 10}, {'source': 'data/actions.csv', 'row': 30}]",True
14,How can organizations assess third-party processes to address GAI risks?,"['Action ID: GV-6.1-006\nSuggested Action: Include clauses in contracts which allow an organization to evaluate third-party\nGAI processes and standards.\nGAI Risks: Information Integrity', 'Action ID: GV-6.2-002\nSuggested Action: Document incidents involving third-party GAI data and systems, including open-\ndata and open-source software.\nGAI Risks: Intellectual Property; Value Chain\nand Component Integration']","Include clauses in contracts that allow organizations to evaluate third-party GAI processes and standards. Additionally, document incidents related to third-party GAI data and systems, including open data and open-source software, to address GAI risks such as Information Integrity, Intellectual Property, Value Chain, and Component Integration.",multi_context,"[{'source': 'data/actions.csv', 'row': 47}, {'source': 'data/actions.csv', 'row': 53}]",True
15,"What steps ensure GAI system deactivation, considering risks to security, value chain, and integration?","['Action ID: GV-1.7-001\nSuggested Action: Protocols are put in place to ensure GAI systems are able to be deactivated when\nnecessary.\nGAI Risks: Information Security; Value Chain\nand Component Integration', 'Action ID: GV-6.2-001\nSuggested Action: Document GAI risks associated with system value chain to identify over-reliance\non third-party data and to identify fallbacks.\nGAI Risks: Value Chain and Component\nIntegration']","Protocols are established to enable the deactivation of GAI systems when required, considering risks related to information security, value chain, and component integration. Additionally, documenting GAI risks associated with the system's value chain helps identify potential over-reliance on third-party data and establish fallback measures.",multi_context,"[{'source': 'data/actions.csv', 'row': 19}, {'source': 'data/actions.csv', 'row': 52}]",True
16,How does threat modeling in GAI systems help anticipate CBRN and info security risks?,"['Action ID: GV-3.2-005\nSuggested Action: Engage in threat modeling to anticipate potential risks from GAI systems.\nGAI Risks: CBRN Information or Capabilities;\nInformation Security', 'Action ID: MS-2.6-007\nSuggested Action: Regularly evaluate GAI system vulnerabilities to possible circumvention of safety\nmeasures.\nGAI Risks: CBRN Information or Capabilities;\nInformation Security']","Threat modeling in GAI systems helps anticipate CBRN and information security risks by identifying potential vulnerabilities and weaknesses in the system that could be exploited to gain access to CBRN information or capabilities, as well as compromise the security of information within the system. By engaging in threat modeling, organizations can proactively assess and address these risks before they are exploited by malicious actors.",multi_context,"[{'source': 'data/actions.csv', 'row': 30}, {'source': 'data/actions.csv', 'row': 130}]",True
17,How is the production of false content defined in relation to AI risks?,"['5 These risks can be further categorized by organizations depending on their unique approaches to risk definition [and management. One possible way to further categorize these risks, derived in part from the UK’s International](https://assets.publishing.service.gov.uk/media/6655982fdc15efdddf1a842f/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf) [Scientific Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction):](https://assets.publishing.service.gov.uk/media/6655982fdc15efdddf1a842f/international_scientific_report_on_the_safety_of_advanced_ai_interim_report.pdf) Confabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; Harmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; Data Privacy; Human-AI Configuration; Obscene, Degrading, and/or Abusive Content; Information Integrity; Information Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual Property. We also note that some risks are cross-cutting between these categories. #### 1. CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious information or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) weapons or other dangerous materials or agents.\n\n 2. Confabulation: The production of confidently stated but erroneous or false content (known colloquially as “hallucinations” or “fabrications”) by which users may be misled or deceived.[6]\n\n 3. Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self-harm or conduct illegal activities. Includes difficulty controlling public exposure to hateful and disparaging or stereotyping content.\n\n 4. Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data.[7]\n\n 5. Environmental Impacts: Impacts due to high compute resource utilization in training or operating GAI models, and related outcomes that may adversely impact ecosystems.']","The production of false content in relation to AI risks is defined as confabulation, which refers to the creation of confidently stated but erroneous or false content that may mislead or deceive users.",reasoning,[{}],True
